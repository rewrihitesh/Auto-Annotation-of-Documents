{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ire_dataset_prep.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO-rf0WtUWWW"
      },
      "source": [
        "#IRE Team Project\n",
        "Project Title: Auto Annotaion of document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9t0CfXV0YVa"
      },
      "source": [
        "# Prepare Dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr9x0uyjvx9z"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exd4EbC90p_P"
      },
      "source": [
        "Upload kaggle.json file to colab, which can be download from Account->api->download "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9jlt1YFJaAT"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ./kaggle.json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_UBw1AtJfnV",
        "outputId": "2e166eee-c3d8-4c0d-f8b5-4cbe5ec12e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqSzt0SBwf7I",
        "outputId": "6d507c7b-7429-4681-c563-d1797691cea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# download kaggle api json file add it to vm \n",
        "!kaggle datasets download -d aashita/nyt-comments"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading nyt-comments.zip to /content\n",
            " 99% 473M/480M [00:02<00:00, 251MB/s]\n",
            "100% 480M/480M [00:03<00:00, 166MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tet9NWFm1AZ7"
      },
      "source": [
        "> Articles*.csv contains the keywords which will be later used in GPT2 for conditional commne genration process.\n",
        "\n",
        "> Commnets*.csv conatins the comment on article with respective Article-ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITW_CQ24wvFU",
        "outputId": "5d0f9059-874f-4978-ef9e-7a4776074bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "!unzip nyt-comments.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  nyt-comments.zip\n",
            "  inflating: ArticlesApril2017.csv   \n",
            "  inflating: ArticlesApril2018.csv   \n",
            "  inflating: ArticlesFeb2017.csv     \n",
            "  inflating: ArticlesFeb2018.csv     \n",
            "  inflating: ArticlesJan2017.csv     \n",
            "  inflating: ArticlesJan2018.csv     \n",
            "  inflating: ArticlesMarch2017.csv   \n",
            "  inflating: ArticlesMarch2018.csv   \n",
            "  inflating: ArticlesMay2017.csv     \n",
            "  inflating: CommentsApril2017.csv   \n",
            "  inflating: CommentsApril2018.csv   \n",
            "  inflating: CommentsFeb2017.csv     \n",
            "  inflating: CommentsFeb2018.csv     \n",
            "  inflating: CommentsJan2017.csv     \n",
            "  inflating: CommentsJan2018.csv     \n",
            "  inflating: CommentsMarch2017.csv   \n",
            "  inflating: CommentsMarch2018.csv   \n",
            "  inflating: CommentsMay2017.csv     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d26TnLuTxfqo",
        "outputId": "983bfb30-5fe7-4915-e323-e06e5b867e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.Series()\n",
        "dataframe"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], dtype: float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-M91I4pxpNQ"
      },
      "source": [
        "# import os\n",
        "# arr = os.listdir('.')\n",
        "# import re\n",
        "# res=[]\n",
        "# for i in arr:\n",
        "#   x = re.search(\"^Comments*\", i)\n",
        "#   if x: res.append(i)\n",
        "# # res\n",
        "# for i in res:\n",
        "#   x=pd.read_csv(i)\n",
        "#   # print(x['commentBody'])\n",
        "#   dataframe=dataframe.append(x['commentBody'],ignore_index=True) \n",
        "#   print(dataframe.describe())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySrAZ51HfeCz",
        "outputId": "ad9ecf58-7c13-4dd2-a1c9-672ee7b20a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "x=pd.read_csv('CommentsMarch2018.csv')\n",
        "y=pd.read_csv('CommentsApril2018.csv')\n",
        "# print(x['commentBody'])\n",
        "dataframe=dataframe.append(x['commentBody'],ignore_index=True)\n",
        "# dataframe=dataframe.append(y['commentBody'],ignore_index=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (14,15,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is \n",
        "lightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC6Q9Hu3HJaH",
        "outputId": "f534d9d5-921d-45df-e481-fa7ebf8efbcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install git+https://github.com/rewrihitesh/gpt-2-simple.git\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting git+https://github.com/rewrihitesh/gpt-2-simple.git\n",
            "  Cloning https://github.com/rewrihitesh/gpt-2-simple.git to /tmp/pip-req-build-icq22uii\n",
            "  Running command git clone -q https://github.com/rewrihitesh/gpt-2-simple.git /tmp/pip-req-build-icq22uii\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (1.18.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (3.0.4)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=24457 sha256=b3a1866e9458427095d6b5e553eb9350c7467c1f4e09a1bafa69295ebe5ea71d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jikr7k45/wheels/f3/60/c4/4bc4e025c301fcd75f4a55bb039ee0d8948b5fa28ccacd71b8\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Thu Oct 29 06:41:29 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it or we can move it to gdrive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell will mount your personal Google Drive in the VM, which later cells can use to get data in/out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "outputId": "b66fcc6f-2914-44fe-c32f-d00e705da2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-MZKSRkpito",
        "outputId": "deb49a4c-e7c9-4c45-d332-e7519ffaaf1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restart the run time \n",
        "% cd drive/My\\ Drive/Colab-Notebooks/ire_final_project"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab-Notebooks/ire_final_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NNsqR4oHb6S",
        "outputId": "13fe73e4-39dc-4772-e12f-fd6f1a9ba4a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 88.3Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 174Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:11, 44.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 241Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 116Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 125Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djMUxsI8Qzef"
      },
      "source": [
        "df=dataframe.to_frame('commentBody')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIxmy36IS9k7"
      },
      "source": [
        "df.to_csv('march18.csv',index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpxVQHEKSpGS",
        "outputId": "4c68a2bb-d888-4ee6-f4f1-04563aaacd5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apr17.csv  checkpoint  march18_april18.csv  march18.csv  models  samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm9E6iz6TXsp"
      },
      "source": [
        "file_name = \"march18_april18.csv\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrXjiujeHcbr",
        "outputId": "c1974edc-b356-42dc-8499-b7d46cf92122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='march18_april18',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS0XXzQhHckn"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='march18_april18')\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='march18_april18')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "# Genrating text and Finetuning\n",
        "\n",
        "> Un-conditional Text Genration\n",
        "\n",
        "GPT2 will genrate the text on its own.\n",
        "\n",
        "> Conditional Text Genration\n",
        "\n",
        "We can pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there.\n",
        "\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "* **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30gBetOHcro"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=100,\n",
        "              temperature=0.8,\n",
        "              top_k=40,\n",
        "              top_p=0.9,\n",
        "              # prefix=\"<|startoftext|>\",\n",
        "              prefix=\"Democratic\",\n",
        "              truncate=\"<|endoftext|>\",\n",
        "              include_prefix=True,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOsCzDibxF3u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}